{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darshan-kale-dsi/Team-05-Bank-Marketing/blob/seb_branch/notebooks/model_adjustment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Adjustment"
      ],
      "metadata": {
        "id": "SIyNiAXg0iPs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "QI9s6UpG0gv8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = pd.read_csv('bank-full.csv', sep = ';')"
      ],
      "metadata": {
        "id": "3LYrWDzi0uHT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trimming extreme values (outliers)\n",
        "dataframe = dataframe[(dataframe['balance']<10000) & (dataframe['duration']<1800)]\n",
        "dataframe['pdays'] = dataframe['pdays'].apply(lambda value: 0 if value==-1 else 1 )\n",
        "dataframe['default'] = dataframe['default'].apply(lambda value: 1 if value =='yes' else 0)\n",
        "dataframe['housing'] = dataframe['housing'].apply(lambda value: 1 if value =='yes' else 0)\n",
        "dataframe['loan'] = dataframe['loan'].apply(lambda value: 1 if value =='yes' else 0)\n",
        "dataframe['y']= dataframe['y'].apply(lambda value: 1 if value =='yes' else 0)\n",
        "dataframe['month'] = dataframe['month'].replace({'may':5,\n",
        "                            'jun':6,\n",
        "                            'jul':7,\n",
        "                            'aug':8,\n",
        "                            'oct':10,\n",
        "                            'nov':11,\n",
        "                            'dec':12,\n",
        "                            'jan':1,\n",
        "                            'feb':2,\n",
        "                            'mar':3,\n",
        "                            'apr':4,\n",
        "                            'sep':9})"
      ],
      "metadata": {
        "id": "XmvTWIp001fT",
        "outputId": "db6f7451-9651-41b5-866c-4c828c6ceaff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-bcfe183c4271>:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  dataframe['month'] = dataframe['month'].replace({'may':5,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "dataframe['day_sine'] = dataframe['day'].apply(lambda value: np.sin(2*value*np.pi/31))\n",
        "dataframe['day_cosine'] = dataframe['day'].apply(lambda value: np.cos(2*value*np.pi/31))\n",
        "\n",
        "dataframe['month_sine'] = dataframe['month'].apply(lambda value: np.sin(2*value*np.pi/12))\n",
        "dataframe['month_cosine'] = dataframe['month'].apply(lambda value: np.cos(2*value*np.pi/12))"
      ],
      "metadata": {
        "id": "8naKgZsu3YX_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = dataframe.drop(['day', 'month'], axis = 1)"
      ],
      "metadata": {
        "id": "4GMW67-Q42pj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = dataframe.drop('y', axis = 1)\n",
        "y = dataframe['y']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size = 0.3,\n",
        "                                                    random_state = 123)"
      ],
      "metadata": {
        "id": "lEnXqtFu6IQ4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def preprocessing_pipeline(dataframe):\n",
        "#   binary_features = [\n",
        "#                       'pdays',\n",
        "#                       'default',\n",
        "#                       'housing',\n",
        "#                       'loan'\n",
        "#                     ]\n",
        "#   numeric_features = [\n",
        "#                       'age',\n",
        "#                       'balance',\n",
        "#                       # 'duration',\n",
        "#                       'campaign',\n",
        "#                       'previous'\n",
        "#                       ]\n",
        "#   time_features = [\n",
        "#                     'day_sine',\n",
        "#                    'day_cosine',\n",
        "#                    'month_sine',\n",
        "#                    'month_cosine'\n",
        "#                    ]\n",
        "#   categorical_features = [\n",
        "#                           'job',\n",
        "#                           'marital',\n",
        "#                           'education',\n",
        "#                           'contact',\n",
        "#                           'poutcome'\n",
        "#                           ]\n",
        "\n",
        "#   dataframe = dataframe[binary_features+numeric_features+time_features+categorical_features]\n",
        "\n",
        "#   preprocessor = ColumnTransformer(transformers=[\n",
        "#       ('num', StandardScaler(), numeric_features),\n",
        "#       ('cat', OneHotEncoder(), categorical_features)\n",
        "#   ])\n",
        "\n",
        "#   dataframe_transformed = preprocessor.fit_transform(dataframe)\n",
        "\n",
        "#   ohe_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
        "#   final_columns = numeric_features + list(ohe_feature_names)\n",
        "#   X_train_preprocessed_df = pd.DataFrame(dataframe_transformed, columns=final_columns)\n",
        "\n",
        "#   X_train_preprocessed_df = pd.concat([X_train_preprocessed_df,dataframe[time_features].reset_index(drop = True)], axis = 1)\n",
        "#   X_train_preprocessed_df = pd.concat([X_train_preprocessed_df,dataframe[binary_features].reset_index(drop = True)], axis = 1)\n",
        "\n",
        "\n",
        "#   return X_train_preprocessed_df"
      ],
      "metadata": {
        "id": "Y2Wk0jlr7cpI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "def preprocessing_pipeline(df):\n",
        "    \"\"\"\n",
        "    Preprocess the input DataFrame by scaling numeric features, one-hot encoding categorical features,\n",
        "    and preserving time and binary features unchanged.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        The input DataFrame containing all necessary columns.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    df_preprocessed : pandas.DataFrame\n",
        "        A DataFrame with transformed numeric and categorical features, along with the original time and binary features.\n",
        "        The columns are ordered as: [scaled numeric features, one-hot encoded categorical features, time features, binary features].\n",
        "    \"\"\"\n",
        "\n",
        "    # Define feature groups\n",
        "    binary_features = [\n",
        "                      # 'pdays',\n",
        "                      'default',\n",
        "                      'housing',\n",
        "                      'loan'\n",
        "                    ]\n",
        "    numeric_features = [\n",
        "                      'age',\n",
        "                      'balance',\n",
        "                      # 'duration',\n",
        "                      'campaign',\n",
        "                      'previous'\n",
        "                      ]\n",
        "    time_features = [\n",
        "                    'day_sine',\n",
        "                   'day_cosine',\n",
        "                   'month_sine',\n",
        "                   'month_cosine'\n",
        "                   ]\n",
        "    categorical_features = [\n",
        "                          'job',\n",
        "                          'marital',\n",
        "                          'education',\n",
        "                          'contact',\n",
        "                          # 'poutcome'\n",
        "                          ]\n",
        "\n",
        "    # Select and copy only the necessary columns\n",
        "    selected_columns = binary_features + numeric_features + time_features + categorical_features\n",
        "    print(selected_columns)\n",
        "    df = df[selected_columns].copy()\n",
        "\n",
        "    # Create a ColumnTransformer to scale numeric features and one-hot encode categorical features\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', StandardScaler(), numeric_features),\n",
        "            ('cat', OneHotEncoder(), categorical_features)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Apply transformations on numeric and categorical features\n",
        "    transformed_array = preprocessor.fit_transform(df)\n",
        "\n",
        "    # Retrieve one-hot encoded feature names\n",
        "    cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
        "\n",
        "    # Build DataFrame from the transformed numeric and categorical features\n",
        "    transformed_feature_names = numeric_features + list(cat_feature_names)\n",
        "    df_transformed = pd.DataFrame(transformed_array, columns=transformed_feature_names, index=df.index)\n",
        "\n",
        "    # Concatenate the unchanged time and binary features\n",
        "    df_preprocessed = pd.concat([df_transformed, df[time_features], df[binary_features]], axis=1)\n",
        "\n",
        "    return df_preprocessed\n"
      ],
      "metadata": {
        "id": "TFjDbXhrLWCZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_processed = preprocessing_pipeline(X_train)\n",
        "X_test_processed = preprocessing_pipeline(X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "pXQnLJWLx26U",
        "outputId": "79c0e562-8a3b-409f-dd40-676df170cd45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['default', 'housing', 'loan', 'age', 'balance', 'campaign', 'previous', 'day_sine', 'day_cosine', 'month_sine', 'month_cosine', 'job', 'marital', 'education', 'contact']\n",
            "here\n",
            "here2\n",
            "['default', 'housing', 'loan', 'age', 'balance', 'campaign', 'previous', 'day_sine', 'day_cosine', 'month_sine', 'month_cosine', 'job', 'marital', 'education', 'contact']\n",
            "here\n",
            "here2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import ensemble as en\n",
        "import boruta as bt\n",
        "x_train_n = X_train_processed.values\n",
        "y_train_n = y_train.values.ravel()\n",
        "\n",
        "# Define model\n",
        "et = en.ExtraTreesClassifier( n_estimators=250, random_state=0, n_jobs=-1 )\n",
        "\n",
        "# Define boruta\n",
        "boruta = bt.BorutaPy( et, n_estimators='auto', verbose=2, random_state=42 ).fit( x_train_n, y_train_n  )"
      ],
      "metadata": {
        "id": "2nmEWKNtM-bI",
        "outputId": "50be85bd-16c2-4add-e266-829ebef9d838",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: \t1 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t33\n",
            "Rejected: \t0\n",
            "Iteration: \t2 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t33\n",
            "Rejected: \t0\n",
            "Iteration: \t3 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t33\n",
            "Rejected: \t0\n",
            "Iteration: \t4 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t33\n",
            "Rejected: \t0\n",
            "Iteration: \t5 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t33\n",
            "Rejected: \t0\n",
            "Iteration: \t6 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t33\n",
            "Rejected: \t0\n",
            "Iteration: \t7 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t33\n",
            "Rejected: \t0\n",
            "Iteration: \t8 / 100\n",
            "Confirmed: \t7\n",
            "Tentative: \t0\n",
            "Rejected: \t26\n",
            "\n",
            "\n",
            "BorutaPy finished running.\n",
            "\n",
            "Iteration: \t9 / 100\n",
            "Confirmed: \t7\n",
            "Tentative: \t0\n",
            "Rejected: \t26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cols_selected = boruta.support_.tolist()\n",
        "# X_train_processed = X_train_processed[X_train_processed.iloc[:, cols_selected].columns.to_list()]\n",
        "# X_test_processed = X_test_processed[X_train_processed.columns]"
      ],
      "metadata": {
        "id": "HtqzfKcQM-XU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import lightgbm as lgb\n",
        "from sklearn import svm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "# model = BernoulliNB()\n",
        "# model = KNeighborsClassifier()\n",
        "# model = lgb.LGBMClassifier(learning_rate=0.09,max_depth=-5,random_state=42)\n",
        "# model = RandomForestClassifier()\n",
        "model = XGBClassifier()\n",
        "# model = XGBClassifier(colsample_bytree = 0.7,\n",
        "#                       gamma = 0.2,\n",
        "#                       learning_rate = 0.1,\n",
        "#                       max_depth = 18,\n",
        "#                       reg_alpha = 0.1,\n",
        "#                       reg_lambda = 100)\n",
        "# model = LogisticRegression()\n",
        "# model = svm.SVC()\n",
        "model.fit(X_train_processed, y_train)\n",
        "y_hat = model.predict(X_test_processed)\n",
        "print(accuracy_score(y_test, y_hat))\n",
        "print(confusion_matrix(y_test, y_hat))"
      ],
      "metadata": {
        "id": "AZbw63zMw7n6",
        "outputId": "ce5403c0-6da4-463e-d999-f87326361300",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9711661522246481\n",
            "[[12519   278]\n",
            " [  105   381]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperopt import tpe, STATUS_OK, Trials, hp, fmin, STATUS_OK, space_eval\n",
        "\n",
        "# Space\n",
        "space = {\n",
        "    'learning_rate': hp.choice('learning_rate', [0.0001,0.001, 0.01, 0.1, 1]),\n",
        "    'max_depth' : hp.choice('max_depth', range(3,21,3)),\n",
        "    'gamma' : hp.choice('gamma', [i/10.0 for i in range(0,5)]),\n",
        "    'colsample_bytree' : hp.choice('colsample_bytree', [i/10.0 for i in range(3,10)]),\n",
        "    'reg_alpha' : hp.choice('reg_alpha', [1e-5, 1e-2, 0.1, 1, 10, 100]),\n",
        "    'reg_lambda' : hp.choice('reg_lambda', [1e-5, 1e-2, 0.1, 1, 10, 100])\n",
        "}\n",
        "\n",
        "# Set up the k-fold cross-validation\n",
        "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
        "\n",
        "\n",
        "# Objective function\n",
        "def objective(params):\n",
        "  xgboost = XGBClassifier(seed=0, **params)\n",
        "  score = cross_val_score(estimator=xgboost,\n",
        "                            X=X_train_processed,\n",
        "                            y=y_train,\n",
        "                            cv=kfold,\n",
        "                            scoring='average_precision',\n",
        "                            n_jobs=-1).mean()\n",
        "  # Loss is negative score\n",
        "  loss = - score\n",
        "  # Dictionary with information for evaluation\n",
        "  return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
        "\n",
        "# Optimize\n",
        "best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = 48, trials = Trials())\n",
        "\n",
        "# Print the values of the best parameters\n",
        "print(space_eval(space, best))"
      ],
      "metadata": {
        "id": "zRIVH9T4IfFc",
        "outputId": "c4ad1074-4eb7-40c9-bc86-a0144647828a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 48/48 [02:15<00:00,  2.83s/trial, best loss: -0.3910000953458241]\n",
            "{'colsample_bytree': 0.5, 'gamma': 0.3, 'learning_rate': 0.01, 'max_depth': 18, 'reg_alpha': 1e-05, 'reg_lambda': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    \"n_estimators\": [100, 200, 300],\n",
        "    \"max_depth\": [3, 5, 7],\n",
        "    \"learning_rate\": [0.1, 0.01, 0.001],\n",
        "    \"subsample\": [0.8, 1.0],\n",
        "    # \"colsample_bytree\": [0.8, 1.0],\n",
        "    # \"min_child_weight\": [1, 3, 5],\n",
        "    # \"gamma\": [0, 1, 5],\n",
        "    # \"reg_alpha\": [0, 1, 10],\n",
        "    # \"reg_lambda\": [0, 1, 10],\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator = model,\n",
        "    param_grid = param_grid,\n",
        "    scoring = \"roc_auc\",\n",
        "    cv = 5,\n",
        "    verbose = 1,\n",
        "    n_jobs = -1\n",
        ")\n",
        "\n",
        "# Fit the grid search\n",
        "final = grid_search.fit(X_train_processed, y_train)\n",
        "y_test = final.predict(X_test_processed)\n",
        "print(accuracy_score(y_test, y_hat))\n",
        "print(confusion_matrix(y_test, y_hat))\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best cross-validation score:\", grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "hQljbT1G40Ys",
        "outputId": "91761733-426b-4540-a023-80e5e0f9c9e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
            "0.990589475269141\n",
            "[[12756    41]\n",
            " [   84   402]]\n",
            "Best parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 300, 'subsample': 1.0}\n",
            "Best cross-validation score: 0.7936587791330002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YcMrcNw_4hdr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fGfE1iiz4hZj"
      },
      "execution_count": 14,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}